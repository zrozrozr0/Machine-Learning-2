{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Recuperar noticias LaJornada**"
      ],
      "metadata": {
        "id": "tnjSWBVTLmsW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEFefcvGLeq0",
        "outputId": "51964ddf-6981-4585-fc4e-6d097be2b252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos guardados en el archivo CSV.\n"
          ]
        }
      ],
      "source": [
        "#V2 fecha normalizada\n",
        "\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "urls_rss = [\n",
        "    \"https://www.jornada.com.mx/rss/deportes.xml?v=1\",\n",
        "    \"https://www.jornada.com.mx/rss/economia.xml?v=1\",\n",
        "    \"https://www.jornada.com.mx/rss/ciencias.xml?v=1\",\n",
        "    \"https://www.jornada.com.mx/rss/cultura.xml?v=1\"\n",
        "]\n",
        "\n",
        "secciones_nombres = {\n",
        "    \"https://www.jornada.com.mx/rss/deportes.xml?v=1\": \"Deportes\",\n",
        "    \"https://www.jornada.com.mx/rss/economia.xml?v=1\": \"Economía\",\n",
        "    \"https://www.jornada.com.mx/rss/ciencias.xml?v=1\": \"Ciencias\",\n",
        "    \"https://www.jornada.com.mx/rss/cultura.xml?v=1\": \"Cultura\"\n",
        "}\n",
        "\n",
        "nombre_archivo_csv = 'LaJornada.csv'\n",
        "encabezados = ['Fuente', 'Titulo', 'Resumen de Contenido', 'Seccion', 'URL', 'Fecha']\n",
        "\n",
        "with open(nombre_archivo_csv, mode='w', newline='', encoding='utf-8') as archivo:\n",
        "    escritor = csv.writer(archivo)\n",
        "    escritor.writerow(encabezados)\n",
        "    for url in urls_rss:\n",
        "        respuesta = requests.get(url)\n",
        "        if respuesta.status_code == 200:\n",
        "            contenido = respuesta.content.decode('utf-8')\n",
        "            raiz = ET.fromstring(contenido)\n",
        "            for elemento in raiz.findall('.//item'):\n",
        "                titulo = elemento.find('title').text\n",
        "                enlace = elemento.find('link').text\n",
        "                descripcion = elemento.find('description').text\n",
        "                fecha_str = elemento.find('pubDate').text\n",
        "                fecha = datetime.strptime(fecha_str, '%a, %d %b %Y %H:%M:%S %Z')\n",
        "                fecha_formateada = fecha.strftime('%d/%m/%Y')\n",
        "                seccion = secciones_nombres[url]\n",
        "                escritor.writerow(['La Jornada', titulo, descripcion, seccion, enlace, fecha_formateada])\n",
        "print('Datos guardados en el archivo CSV.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recuperar noticias Expansión**"
      ],
      "metadata": {
        "id": "jw4eUppILvkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#V2 Fecha normalizada\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "urls_rss = [\n",
        "    \"https://expansion.mx/rss/economia.xml\",\n",
        "    \"https://expansion.mx/rss/tecnologia\",\n",
        "]\n",
        "\n",
        "nombre_archivo_csv = 'Expansion.csv'\n",
        "\n",
        "encabezados = ['Fuente', 'Titulo', 'Resumen de Contenido', 'Seccion', 'URL', 'Fecha']\n",
        "\n",
        "encabezados_peticion = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "}\n",
        "\n",
        "espacios_nombres = {\n",
        "    'dc': 'http://purl.org/dc/elements/1.1/',\n",
        "    'content': 'http://purl.org/rss/1.0/modules/content/',\n",
        "    'media': 'http://search.yahoo.com/mrss/'\n",
        "}\n",
        "\n",
        "with open(nombre_archivo_csv, mode='w', newline='', encoding='utf-8') as archivo:\n",
        "    escritor = csv.writer(archivo)\n",
        "    escritor.writerow(encabezados)\n",
        "    for url in urls_rss:\n",
        "        respuesta = requests.get(url, headers=encabezados_peticion)\n",
        "        if respuesta.status_code == 200:\n",
        "            contenido = respuesta.content.decode('utf-8')\n",
        "            raiz = ET.fromstring(contenido)\n",
        "            for elemento in raiz.findall('.//item', espacios_nombres):\n",
        "                titulo = elemento.find('title').text if elemento.find('title') is not None else \"Sin título\"\n",
        "                enlace = elemento.find('link').text if elemento.find('link') is not None else \"Sin link\"\n",
        "                descripcion = elemento.find('description').text if elemento.find('description') is not None else \"Sin descripción\"\n",
        "                fecha_str = elemento.find('pubDate').text if elemento.find('pubDate') is not None else \"Sin fecha\"\n",
        "\n",
        "                # Formatear la fecha\n",
        "                fecha = datetime.strptime(fecha_str, '%a, %d %b %Y %H:%M:%S %Z')\n",
        "                fecha_formateada = fecha.strftime('%d/%m/%Y')\n",
        "\n",
        "                seccion = elemento.find('category').text if elemento.find('category') is not None else \"Sin Cat\"\n",
        "\n",
        "                escritor.writerow(['Expansión', titulo, descripcion, seccion, url, fecha_formateada])\n",
        "        else:\n",
        "            print(f\"Error al acceder URSS: {respuesta.status_code} en URL {url}\")\n",
        "\n",
        "print('Datos guardados en el archivo CSV.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtFvRr8BLvQp",
        "outputId": "b7eadea3-8641-488f-80d1-bc2fa057744d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos guardados en el archivo CSV.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generar CSV normalizado**"
      ],
      "metadata": {
        "id": "dZ29dlU2L4Ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#por si se reinicia\n",
        "!python -m spacy download es_core_news_sm\n",
        "!pip install spacy\n",
        "!pip install torch\n",
        "# Descargar las stopwords en español de NLTK\n",
        "nltk.download('stopwords')\n",
        "# Descargar el lematizador de NLTK\n",
        "nltk.download('wordnet')\n",
        "\n"
      ],
      "metadata": {
        "id": "n2hmf5SyMDHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#V2 CODIGO con libreria nltk\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Descargar las stopwords en español de NLTK\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "# Cargar el archivo CSV\n",
        "df = pd.read_csv('Expansion.csv', sep=',', header=None, engine='python')\n",
        "\n",
        "# Función para normalizar el texto\n",
        "def normalize_text(text):\n",
        "    tokens = text.split()\n",
        "    normalized_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.lower() not in stop_words:\n",
        "            normalized_tokens.append(token)\n",
        "    return ' '.join(normalized_tokens)\n",
        "\n",
        "# Normalizar el texto de las columnas 'Título' y 'Resumen de Contenido'\n",
        "df[1] = df[1].apply(normalize_text)  # Título\n",
        "df[2] = df[2].apply(normalize_text)  # Resumen de Contenido\n",
        "\n",
        "# Guardar el dataframe modificado en un nuevo archivo CSV\n",
        "df.to_csv('normalized_data_corpus.csv', index=False, header=None)\n",
        "print('Datos guardados en el archivo CSV.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYSZv-yEL-aJ",
        "outputId": "54f388d1-abe0-49aa-927a-ecf9a332df06"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos guardados en el archivo CSV.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#V3 normalizado y lematizado\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Cargar el archivo CSV\n",
        "df = pd.read_csv('Expansion.csv', sep=',', header=None, engine='python')\n",
        "\n",
        "# Función para normalizar el texto\n",
        "def normalize_text(text):\n",
        "    tokens = text.split()\n",
        "    normalized_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.lower() not in stop_words:\n",
        "            normalized_tokens.append(lemmatizer.lemmatize(token.lower()))  # Lemmatization\n",
        "    return ' '.join(normalized_tokens)\n",
        "\n",
        "# Normalizar el texto de las columnas 'Título' y 'Resumen de Contenido'\n",
        "df[1] = df[1].apply(normalize_text)  # Título\n",
        "df[2] = df[2].apply(normalize_text)  # Resumen de Contenido\n",
        "\n",
        "# Guardar el dataframe modificado en un nuevo archivo CSV\n",
        "df.to_csv('normalized_data_corpus.csv', index=False, header=None)\n",
        "print('Datos guardados en el archivo CSV.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6GieEOeN6eZ",
        "outputId": "21e127cc-2b2f-4b91-f793-306851b0e040"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos guardados en el archivo CSV.\n"
          ]
        }
      ]
    }
  ]
}